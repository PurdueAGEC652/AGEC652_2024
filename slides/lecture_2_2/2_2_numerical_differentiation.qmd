---
title: "AGEC 652 - Lecture 2.2"
subtitle: "Numerical differentiation"
author: "Diego S. Cardoso"
institute: "Purdue University"
execute:
  echo: true
format:
  revealjs: 
    theme: [white, ./../agec_652_style.css]
    slide-number: c
    show-slide-number: all
    code-block-bg: true
    code-block-border-left: "#31BAE9"
    code-copy: hover
    fig-width: 8
    fig-height: 4
    chalkboard:
      theme: whiteboard
      chalk-effect: 0.1
editor:
  render-on-save: false
---



```{julia}
#| echo: false
using Pkg
Pkg.activate(".")
Pkg.instantiate()
Pkg.add("ForwardDiff"); Pkg.add("FiniteDifferences"); Pkg.add("Symbolics")
```



## Course Roadmap {background-color="gold"}

1.  [Introduction to Scientific Computing]{.gray}
2.  **Fundamentals of numerical methods**
    1.  [Numerical arithmetic]{.gray}
    2.  **Numerical differentiation**
    3.  Numerical integration
3.  Systems of equations
4.  Optimization
5.  Structural estimation

## Agenda {background-color="gold"}

- Today we will learn how to take derivatives using the computer
- Precision plays a big role in this type of processing, so we will start by reviewing a mathematical concept to express orders of magnitude
- Then, we will move to methods to calculate derivatives
- And end with packages to help us calculate numerical (and even analytical) derivatives



## Main references for today {background-color="gold"}

-   Miranda & Fackler (2002), Ch. 5
-   Judd (1998), Ch. 3
-   Nocedal & Wright (2006), Ch. 8 
-   Lecture notes for Ivan Rudik's *Dynamic Optimization* (Cornell) 


# Big O notation

*A useful way to represent limiting behavior*

## Big O notation

How do we quantify **speed** and **accuracy** of computational algorithms?

. . .

The usual way is to use **Big O (or order) notation**

. . .

- General mathematical definition: describes the *limiting behavior of a function* when the argument tends towards a particular value or infinity
  - You've seen this before in the expression of Taylor series' errors

. . .

- Programming context: describes the limiting behavior of algorithms in terms of run time/memory/accuracy as input size grows


## Big O notation

Written as: $O(f(x))$

Formally, for two real-valued functions $f(x)$ and $g(x)$ we say

$$g(x) = O(f(x))$$

if there $\exists$ a positive $C$ such that 

$$|g(x)| \leq C |f(x)|, \text{as}\; x \rightarrow a$$

. . .

So $g$ is bounded by $f$ up to a scalar. Another way of saying it (if $g$ is non-zero): $\lim_{x \rightarrow a} \frac{|g(x)|}{|f(x)|} < \infty$

Intuitively, $g$ never gets "too far away" from $f$ as $x \rightarrow a$



## Big O notation

Here is how to think about it:

$\mathbf{O(x)}$: **linear**

- Time to solve increases linearly in input x
- Accuracy changes linearly in input x

. . .

*Any Examples?*

. . .

- Time to find a particular value in an unsorted array
  - For each element, check whether it is the value we want



## Big O notation

$\mathbf{O(c^x)}:$ **exponential**

- Time to solve increases exponentially in input x
- Accuracy changes exponentially in input x

. . .

*Any examples?*

. . .

- Time to solve a standard dynamic program, e.g. traveling salesman
  - For each city $i=1,...,n$, solve a Bellman equation as a function of all other cities



## Big O notation

$\mathbf{O(n!)}$: factorial

- Time to solve increases factorially in input x
- Accuracy changes factorially in input x

. . .

*Any examples?*

. . .

- Solving traveling salesman by brute force
  - Obtain travel time for all possible combinations of intermediate cities



## Big O notation: accuracy example

This is how you have probably seen Big O used before:

. . .

Taylor series for $sin(x)$ around zero:

$sin(x) = x - x^3/3! + x^5/5! + O(x^7)$

What does $O(x^7)$ mean here?



## Big O notation: accuracy example

$sin(x) = x - x^3/3! + x^5/5! + O(x^7)$

. . .

As we move away from $0$ to some $x$, the upper bound of the growth rate in the error of our approximation to $sin(x)$ is $x^7$

If we are approximating about zero so $x$ is small: $x^n$ is decreasing in $n$

. . .

For small $x$, higher order polynomials mean the error will grow slower and we have a better local approximation



## Taylor expansions

```{julia, results = 'hide'}
# fifth and third order Taylor approximations
sin_error_5(x) = sin(x) - (x - x^3/6 + x^5/120)
sin_error_3(x) = sin(x) - (x - x^3/6)
```

. . .

```{julia, echo = TRUE}
println("Error of fifth-order approximation at x = .001 is: $(sin_error_5(.001))
Error of third-order approximation at x = .001 is: $(sin_error_3(.001))
Error of fifth-order approximation at x = .01 is: $(sin_error_5(.01))
Error of third-order approximation at x = .01 is: $(sin_error_3(.01))
Error of fifth-order approximation at x = .1 is: $(sin_error_5(.1))
Error of third-order approximation at x = .1 is: $(sin_error_3(.1))")
```



## Big O notation: speed examples

Here are a few examples for fundamental computational methods


## Big O notation: speed examples

$\mathbf{O(1)}$: algorithm executes in **constant time**

The size of the input does not affect execution speed

. . .

Example: 

. . .

- Accessing a specific location in an array: `x[10]`



## Big O notation: speed examples

$\mathbf{O(x)}$: algorithm executes in **linear time**

Execution speed grows linearly in input size

Example:

. . .

- Inserting an element into an arbitrary location in a 1 dimensional array
  - Bigger array $\rightarrow$ need to shift around more elements in memory to accommodate the new element



## Big O notation: speed examples

$\mathbf{O(x^2):}$ algorithm executes in **quadratic time**

- More generally called **polynomial time** for $O(x^n)$

Execution speed grows quadratically in input size

Example: 

. . .

- *Bubble sort*: step through a list, compare adjacent elements, swap if in the wrong order


# Numerical differentiation

## Differentiation

Derivatives are obviously important in economics for finding optimal allocations, etc

The formal definition of a derivative is:

. . .

$$\frac{d f(x)}{dx} = \lim_{h\rightarrow 0} \frac{f(x+h)-f(x)}{h}$$

. . .

But we can let $t = 1/h$ and reframe this as an infinite limit

. . .

$$\frac{d f(x)}{dx} = \lim_{t\rightarrow \infty} \frac{f(x+1/t)-f(x)}{1/t}$$

which we know a computer can't handle because of finite space to store $t$



## Computer differentiation

How do we perform derivatives on computers if we can't take the limit?

. . .

**Finite difference methods**

Idea: Approximate the limit by letting $h$ be a small number

. . .

What does a finite difference approximation look like?



## Forward difference

The forward difference looks exactly like the formal definition without the limit: $$\frac{d f(x)}{dx} \approx \frac{f(x+h)-f(x)}{h}$$

. . .

It works the same for partial derivatives: 

$$\frac{\partial g(x,y)}{\partial x} \approx \frac{g(x+h,y)-g(x,y)}{h}$$


## Forward difference

Let's see how it works in practice by calculating derivatives of $x^2$ at $x=2$

1. We will write a function `deriv_x_squared(h,x)` that returns the forward difference approximation to the derivative of $x^2$ around with step size $h$
2. Then, we evaluate the function for `x=2` and 3 values of `h`
  1. `h = 1e-1`
  2. `h = 1e-12`
  3. `h = 1e-30`


## Forward difference

```{julia, results = 'hide'}
deriv_x_squared(h,x) = ((x+h)^2 - x^2)/h # derivative function
```

. . .

```{julia}
println("
        The derivative with h=1e-1  is: $(deriv_x_squared(1e-1 ,2.))
        The derivative with h=1e-12 is: $(deriv_x_squared(1e-12,2.))
        The derivative with h=1e-30 is: $(deriv_x_squared(1e-30,2.))")
```



## Error, it's there

None of the values we chose for $h$ were perfect, but clearly some were better than others

. . .

Why?

. . .

We face two opposing forces:

- We want $h$ to be as small as possible so that we can approximate the limit as well as we possibly can, *BUT*

. . .

- If $h$ is small then $f(x+h)$ is close to $f(x)$, we can run into rounding issues like we saw for $h=10^{-30}$



## Error, it's there

We can select $h$ in an optimal fashion: $h = \max\{|x|,1\}\sqrt{\epsilon}$

. . .

There's proofs for why this is the case but generally testing out different $h$'s works fine



## How much error is in a finite difference?

Can we measure the error growth rate in $h$ (i.e. Big O notation)?

. . .

Perform a first-order Taylor expansion of $f(x)$ around $x$:

. . .

$$f(x+h) = f(x) + f'(x)h + O(h^2)$$

Recall that $O(h^2)$ means the error in our approximation grows quadratically in $h$, though we only did a linear approximation

. . .

How can we use this to understand the error in our finite difference approximation?



## How much error is in a finite difference?

Rearrange to obtain: $f'(x) = \frac{f(x+h) - f(x)}{h} + O(h^2)/h$

. . .

$\Rightarrow f'(x) = \frac{f(x+h) - f(x)}{h} + O(h)$ because $O(h^2)/h = O(h)$

. . .

**Forward differences have linearly growing errors**

. . .

If we halve $h$, we halve the error in our approximation (ignoring rounding/truncation issues)



## Improvements on the forward difference

How can we improve the accuracy of the forward difference?

. . .

First, *why* do we have error?

. . .

- Because we are approximating the slope of a tangent curve at $x$ by a secant curve passing through $(x,x+h)$
  - The secant curve has the average slope of $f(x)$ on $[x,x+h]$

. . .

- We want the derivative at $x$, which is on the edge of $[x,x+h]$

*How about we center* $x$?



## Central differences

We can approximate $f'(x)$ in a slightly different way: $$f'(x) \approx \frac{f(x+h)-f(x-h)}{2h}$$

. . .

- This leaves $x$ in the middle of the interval over which we are averaging the slope of $f(x)$

. . .

*Is this an improvement on forward differences?*



## How much error is in a central finite difference?

Lets do two second-order Taylor expansions:

- $f(x+h) = f(x) + hf'(x) + h^2/2! f''(x) + O(h^3)$
- $f(x-h) = f(x) - hf'(x) + (-h)^2/2! f''(x) + O(h^3)$

. . .

Subtract the two expressions (note that $O(h^3) - O(h^3) = O(h^3)$) and then divide by $2h$ to get

. . .

$$f'(x) = \frac{f(x+h)-f(x-h)}{2h} + O(h^2)$$


## How much error is in a central finite difference?

$$f'(x) = \frac{f(x+h)-f(x-h)}{2h} + O(h^2)$$

. . .

Error is quadratic in $h$: if we halve $h$ we reduce error by 75%



## Higher-order approximations

- So central differencing sounds like a good idea: add one more point to improve accuracy!
- Can we keep doing that? Using more points around $x$?

. . .

- Yes! Let's add two more points, one on each side of $x$: $x-2h$ and $x+2h$
  - Let's do a fourth-order Taylor expansion this time

. . .

- $f(x+h) = f(x) + 2hf'(x)  +   (h)^2/2!  f''(x)   + (h)^3/3! f'''(x) +   (h)^4/4! f''''(x) + O(h^5)$
- $f(x-h) = f(x) - 2hf'(x)  +  (-h)^2/2!  f''(x)  + (-h)^3/3! f'''(x) +  (-h)^4/4! f''''(x) + O(h^5)$
- $f(x+2h) = f(x) + 2hf'(x) +  (2h)^2/2!  f''(x)  + (2h)^3/3! f'''(x) +  (2h)^4/4! f''''(x) + O(h^5)$
- $f(x-2h) = f(x) - 2hf'(x) + (-2h)^2/2!  f''(x) + (-2h)^3/3! f'''(x) + (-2h)^4/4! f''''(x) + O(h^5)$


## Higher-order approximations

We are looking for a combination of coefficients $a, b, c, d$ in 

$$af(x+h) + bf(x-h) + cf(x+2h) + df(x-2h)$$

such that we can cleverly cancel out higher order (2nd, 3rd, and 4th) terms

. . .

If you do the math, you'll find that with $a = -1/12, b = 2/3, c = -2/3, d = 12$ we get

$$f'(x) = \frac{-f(x+2h)+8f(x+h)-f(x-h)+f(x-2h)}{12h} + O(h^4)$$

**By adding yet another two points, we quadrupled the accuracy**: if we halve $h$ we reduce error by 93.75%!

. . .

- You can keep doing this to improve accuracy, but the math gets complicated pretty quickly. And there's gotta be a trade-off, right?


## Why use anything but central differences?

Suppose we're computing a Jacobian of a multidimensional function. Why would we ever use forward differences instead of central differences?

. . .

- For each central difference, we need to compute $g(x_1-h,x_2,...)$ and $g(x_1+h,x_2,...)$ for each $x_i$

. . .

- But for a forward difference we only need to compute $g(x_1,x_2,...)$ once and then $g(x_1+h,x_2,...)$ for each $x_i$

. . .

Forward differences saves on the *number of operations at the expense of accuracy*

- For high dimensional functions it may be worth the trade-off



## Higher order finite differences

We can use these techniques to approximate higher order derivatives

. . .

For example, take two third order Taylor expansions

. . .

- $f(x+h) = f(x) + f'(x)h + f''(x)h^2/2! + f'''(x)h^3/3! + O(h^4)$
- $f(x-h) = f(x) + f'(x)(-h) + f''(x)(-h)^2/2! + f'''(x)(-h)^3/3! + O(h^4)$

. . .

Add the two expressions and then divide by $h^2$ to get

. . .

$$f''(x) = \frac{f(x+h) - 2f(x) + f(x-h)}{h^2} + O(h^2)$$

. . .

Second derivatives are important for calculating Hessians and checking maxima or minima


## Packages for finite differencing 

- We learned how to program simple derivatives using the principles of finite differencing. But there's a lot to figure out:
  - Can we pick an optimal $h$ depending on the function?
  - What if we want to use more than two points in central differencing? It's gotta be pretty complicated to program
  - And higher-order derivaties seem a pain to program too...

. . .

**Despair not!** That's what (reliable) packages are for. *And we thank the good souls who programmed those tools for free so we can focus on solving our research questions*

- For finite differencing, we will pick package `FiniteDifferences.jl`
  

## Using `FiniteDifferences.jl`: basics

- This package gives us the tools to create our customized **differentiation functions**
  - It means that it will return a function itself that will perform finite differentiation based on the method and parameters we specify

Let's start with the scalar (univariate) case

- There are three types of methods: 
  - `forward_fdm(p, q)`: forward finite differences method
  - `backward_fdm(p, q)`: forward finite differences method
  - `central_fdm(p, q)`: forward finite differences method
- Each of these functions take as minimum arguments
  - $p$: The order of approximation (number of points you want to use)  
  - $q$: The order of the derivative (e.g., `1` for first-order )
- *Note that, by default, you don't need to specify* $h$: *it figures out the optimal one based on an adaptative algorithm*

## Using `FiniteDifferences.jl`: scalars

Let's create an operator for 1st-order differentation with each type using the simplest case with 2 points

```{julia}
using FiniteDifferences # Import package
my
```



# Automatic differentiation


## Differentiation without error?

Finite differences put us in between two opposing forces on the size of $h$

. . .

Can we improve upon finite differences?

. . .

- Analytic derivatives
  - One way is to code up the actual derivative

. . .

```{julia, results = 'hide'}
deriv_x_squared(x) = 2x
```

```{julia, echo = F}
println("The deriviative is: $(deriv_x_squared(2.))")
```

. . .

Exact solution! *(But up to machine precision, of course)*



## Analytic derivatives

Coding up analytic derivatives by hand for complex problems is not always great because:

. . .

- It can take A LOT of programmer time, more than it is worth

. . .

- Humans are susceptible to error in coding or calculating the derivative mathematically^[But Julia can actually solve some derivates for us... more on that in the Appendix.]

. . .

There is another option...



## Autodiff: let the computer do it

Think about this: your code is *always* made up of simple arithmetic operations

- add, subtract, divide, multiply
- trig functions
- exponentials/logs
- etc

. . .

The closed form derivatives of these operations is not hard: it turns out your computer can do it and yield exact solutions

That's called **automatic differentiation**, or **autodiff** for short



## Autodiff: let the computer do it

How? 

. . .

There are methods that basically apply a giant *chain rule* to your whole program and break down the derivative into the (easy) component parts that another package knows how to handle

. . .

The details of decomposing calculations with computer instructions get pretty complicated pretty fast. We're not going to code it by hand. Instead, we're going to use a package for that: `ForwardDiff`

- The name follows from *forward mode*, which is one way of doing autodiff
- Check out Nocedal & Wright Ch.8 for more details



## Autodiff: let the computer do it

```{julia, results = 'hide'}
# Your function needs to be written in a particular way because the autodiff package can
# only accept arrays as arguments. But it's easy to adapt it to work with scalars: you
# just need to define an array with one element
ff(x) = (x[1])^2 # x^2
x = [2.0]; # location to evaluate: ff(x) = 2^2
```

. . .

```{julia}
using ForwardDiff # This is the package that has autodiff
g(f,x) = ForwardDiff.gradient(f,x); # Define g = ∇f for a generic function
println("ff'(x) at $(x[1]) is: $(g(ff,x)[1])") # display gradient value
```

**Exact solution!**



## Autodiff: let the computer do it

Once you get the hang of coding up function for autodiff it's not that hard

```{julia, results = 'hide'}
fff(x) = sin(x[1]^2) # write it in the autodiff way
x = [2.0]  # location to evaluate: ff(x) = 2^2
g(f,x) = ForwardDiff.gradient(f,x)  # g = ∇f
```

. . .

```{julia}
println("fff'(x) at $(x[1]) is: $(g(fff,x)[1])") # display gradient value
```




## .blue[Autodiff: let the computer do it]

Try it out! Code up the derivative of $log(10 + x)$ at `x=5.5` using automatic differentiation

1. Make sure you have the `ForwardDiff` package
  1. If not type `using Pkg` then `Pkg.add("ForwardDiff")`
2. Define your function `my_fun`
3. Define `x` as a 1-element array containing `5.5`
4. Define the gradient operator `g` using `ForwardDiff.gradient(f,x)`
5. Use `g` to evaluate the derivative
6. Derive the analytic solution and compare to your numeric one


# Appendix: Symbolics

*Analytical differentiation in Julia*


## Programming analytical derivatives?

- We've seen the obvious benefits of being able to calculate numerical derivaties
  - Easily differentiate complicated functions
  - Easily get Jacobians and Hessians for functions of MANY variables

. . .

- So what's the point of analytical derivatives?
  - Well... just because we can!
  - It might not be helpful for big numerical problems, but it can **speed up tedious derivations** that you can program
  - Or, if nothing else, it can help you check your math

## Enter symbolic calculation

- Analytical derivatives (and general manipulation of mathematical expressions) is typically done in computers using a **CAS: Computer Algebra System**
  - Mathematica and Maple are some examples of systems like this
- As it turns out, you can also do that in Julia using the package `Symbolics.jl`
- This appendix is just a short tutorial on how to perform analytical operations (not only derivatives)
  - Since most times we want to take those expressions and use them in papers and assignments, we will also use package `Latexify.jl` to generate LaTeX expressions for us



