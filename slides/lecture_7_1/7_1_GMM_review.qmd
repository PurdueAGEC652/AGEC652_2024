---
title: "AGEC 652 - Lecture 7.1"
subtitle: "GMM: Theory review"
author: "Diego S. Cardoso"
institute: "Purdue University"
execute:
  echo: true
  cache: true
format:
  revealjs: 
    theme: [white, ./../agec_652_style.css]
    slide-number: c
    show-slide-number: all
    code-block-bg: true
    code-block-border-left: "#31BAE9"
    code-copy: hover
    fig-width: 8
    fig-height: 4
    chalkboard:
      theme: whiteboard
      chalk-effect: 0.1
editor:
  render-on-save: false
---

```{julia}
#| include: false
using Pkg
Pkg.activate(".")
Pkg.instantiate()
# Pkg.add("JuMP")
# Pkg.add("Ipopt")
# Pkg.add("Statistics")
# Pkg.add("Plots")
# Pkg.add("ForwardDiff")
# Pkg.add("LinearAlgebra")
```



## Course Roadmap {background-color="gold"}


1.  [Introduction to Scientific Computing]{.gray}
2.  [Fundamentals of numerical methods]{.gray}
3.  [Systems of equations]{.gray}
4.  [Optimization]{.gray}
5.  [Structural estimation: Intro]{.gray}
6.  [Maximum Likelihood Estimator]{.gray}
7.  **Generalized Method of Moments**
8.  Simulation-based methods


## Main references for today {background-color="gold"}

- Cameron & Trivedi (2008), Greene (2018)
- Course materials by Michael Delgado (Purdue), Matt Woerman (UMass) and Simon Quinn (Oxford)


# Intro to GMM

## Generalized Method of Moments in one slide

1. Our economic model defines the following population moment conditions: at the true parameter $\theta_0$, $g(x;\theta)$ are on average equal to zero
$$E[g(x;\theta_0)] = 0$$

. . .

2. We observe some data $x_i, i=1,\dots,N$ and calculate sample analogue 
$$E[g(x;\theta)] \approx \frac{1}{N} \sum_{i=1}^N g(x_i;\theta) \equiv g_N(\theta)$$

. . .

3. The GMM estimate is given by ( $W_N$ is a weighting matrix)
$$\hat{\theta}_{GMM} = \arg \min_\theta g_N(\theta)^\prime \; W_N \; g_N(\theta)$$



## (Generalized) Method of Moments intuition

Suppose we draw five numbers from an unknown distribution
$$y = {47.3, 51.2, 50.5, 44.9, 53.1}$$

. . .

Suppose this unknown distribution has mean $\mu$, giving a population moment condition

$$E[y_i] = \mu \Rightarrow E[y_i - \mu] = 0$$

. . .

We expect the population moment condition to also hold in the sample analogue

$$\frac{1}{N}\sum_{i=1}^N (y_i - \mu) = 0$$



## (Generalized) Method of Moments intuition

Forget the weighting matrix for now. We have one condition and one parameter, so this is effectively a special case: just *Method of Moments*

. . .

For our estimate, we pick $\hat{\mu}$ that minimizes

$$\left(\frac{1}{N}\sum_{i=1}^N (y_i - \hat{\mu})\right)^2 $$

In this simple case, this is just solving for $\frac{1}{N}\sum_{i=1}^N (y_i - \hat{\mu}) = 0$




## Generalized Method of Moments example

Linear regression: again, we have $Y_{it} = X_i\beta + \epsilon_i$. But now, instead of normality, we assume $\epsilon_i$ is orthogonal to data $X_i$ (a $1 \times K$ vector)
$$E[x_{ki} \epsilon_i] = 0 \Rightarrow E[x_{ki} (Y_i - X_i\beta)] = 0$$

- This actually gives $K$ moment conditions: one for each variable $x_{ki} \in X_i$

. . .

We replace these $K$ conditions with their respective sample analogues and solve for

$$\frac{1}{N}\sum_{i=1}^N x_{ki} (Y_i - X_i \hat{\beta}) = 0$$

. . .

- We can show that this solution is analytically equivalent to OLS, too

# GMM theory

## GMM: General case


1. Start with data $z_1, \dots, z_N$ drawn from a population with $M$ moment conditions that are functions of vector $\theta$ with $K \leq M$ parameters^[The "generalized" in GMM comes from allowing more moment conditions than parameters; the "standard" Method of Moments requires M=K]
$$E[g(Z; \theta)] = 0$$


. . .

Where do moment conditions come from?

- Economic model conditions: first-order optimality, market clearing, zero arbitrage, etc 
- Statistical assumptions: error orthogonality $(E[x\epsilon]=0)$
- Instruments orthogonality $(E[z\epsilon]=0)$
- Model fit: predicted market shares are equal to realized market shares



## GMM: General case

2. Construct empirical (sample analogue) moment conditions
$$\frac{1}{N} \sum_{i=1}^N g(z_i; \theta) = 0$$



## GMM: General case


3. Compute the GMM estimate

$$\hat{\theta}_{GMM} = \arg \min_\theta Q_N(\theta), \; Q_N(\theta) = \left[\frac{1}{N} \sum_{i=1}^N g(z_i; \theta)\right]^\prime W \left[\frac{1}{N} \sum_{i=1}^N g(z_i; \theta)\right]$$

. . .

- If $M = K$ and the problem is well-conditioned, then $\frac{1}{N} \sum_{i=1}^N g(z_i; \theta) = 0$ is $K\times K$ (non)linear system and we can find the $\hat{\theta}$ that solves it

. . .

- But if $M > K$, we almost certainly can't find $K$ parameters that satisfy more that $K$ conditions simultaneously
  - So we look for parameters that get as close as possible to satisfying all moment conditions $\rightarrow$ we minimize deviations from zero, weighted by a $M\times M$ matrix $W$




## GMM: General case

4. Compute $V(\hat{\theta}_{GMM})$, the variance-covariance matrix of the estimates

- More on that soon



## Properties of GMM

Under some *regularity conditions*, GMM has the following properties


1. Consistency
2. Asymptotic normality

. . .

- Note that, unlike MLE, GMM is not asymptotically efficient

These properties require some assumptions on the empirical moments



## Properties of GMM: empirical moments assumption

We assume the following about empirical moments at the true parameter value, $\theta_0$

1. Empirical moments obey the law of large numbers
$$\frac{1}{N} \sum_{i=1}^N g(z_i; \theta_0) \stackrel{p}{\rightarrow} 0$$

. . .

2. The derivatives of the empirical moments converge to the $M \times K$ Jacobian matrix
$$D_N(\theta_0) = \frac{1}{N} \sum_{i=1}^N \left.\frac{\partial g(z_i; \theta)}{\partial \theta^\prime}\right|_{\theta=\theta_0} \stackrel{p}{\rightarrow}  D_0 \equiv D(\theta_0) = E\left[\frac{\partial g(z_i; \theta_0)}{\partial \theta_0^\prime}\right]$$



## Properties of GMM: empirical moments assumption

3. Empirical moments obey the central limit theorem

$$\sqrt{N}\frac{1}{N} \sum_{i=1}^N g(z_i; \theta_0) \stackrel{d}{\rightarrow} \mathcal{N}(0, S_0)$$

where $S_0 = E[g(z_i; \theta_0) g(z_i; \theta_0)^\prime]$ is the variance-covariance matrix of moments (an $M\times M$ matrix)

. . .

- We also need to assume that the weighting matrix converges to $W_0$, a finite symmetric positive definite matrix

$$W \stackrel{p}{\rightarrow}W_0$$



## Properties of GMM: consistency
$$\hat{\theta}_{GMM} \stackrel{p}{\rightarrow} \theta_0$$

As sample size grows to infinity, $\hat{\theta}_{GMM}$ gets arbitrarily close to the true parameter value, $\theta_0$



## Properties of GMM: Asymptotic normality

. . .

$$\sqrt{N}(\hat{\theta}_{GMM}-\theta_0) \stackrel{d}{\rightarrow} \mathcal{N}\left(0, V_0\right)$$

where $V_0$ has a typical *sandwich form*

$$V_0 = \underbrace{(D_0^\prime W_0 D_0)^{-1}}_{\text{bread}}\underbrace{(D_0^\prime W_0 S_0 W_0 D_0)}_{\text{filling}} \underbrace{(D_0^\prime W_0 D_0)^{-1}}_{\text{bread}}$$

. . .

As the sample size grows to infinity, the distribution of $\hat{\theta}_{GMM}$ converges to a normal distribution with mean as the true parameter value and a particular Variance-Covariance structure




## GMM variance estimator

Any valid weighting matrix $W$ yields a consistent GMM estimator

But the choice of $W$ affects variance, so we want to use some optimal $W$ that minimizes the variance of the estimator


. . .

It can be shown that the **optimal weigthing matrix** is given by

$$W_0 = S_0^{-1} = \left\{E[g(z_i; \theta_0) g(z_i; \theta_0)^\prime]\right\}^{-1}$$

which yields^[This is the "non-robust" VCOV matrix, i.e., assuming homoskedasticity and no clustering/residual correlation structure. Check references to see how to construct robust versions.]

$$V(\hat{\theta}_{GMM}) = (D_0^\prime S_0^{-1} D_0)^{-1}$$



## Computing $\hat{\theta}_{GMM}$

This is the objective function for GMM 

$$\hat{\theta}_{GMM} = \arg \min_\theta Q_N(\theta), \; Q_N(\theta) = \left[\frac{1}{N} \sum_{i=1}^N g(z_i; \theta)\right]^\prime W \left[\frac{1}{N} \sum_{i=1}^N g(z_i; \theta)\right]$$

Once again, we can use numerical optimization to calculate $\hat{\theta}_{GMM}$

. . .

But what $W$ do we use?



## Computing $\hat{\theta}_{GMM}$

Technically, the GMM estimator is consistent for any symmetric positive definite matrix

But different matrices give different variances $\rightarrow$ we want to pick $W$ that minimizes variance

. . .

This is, the sample analogue of the **optimal weigthing matrix**

$$\hat{W} = \hat{S}^{-1} = \left\{E[g(z_i; \hat{\theta}) g(z_i; \hat{\theta})^\prime]\right\}^{-1}$$

. . .

But we have a chicken and egg problem here: we need $\hat{W}$ to estimate $\hat{\theta}$ but we need $\hat{\theta}$ to get $\hat{W}$!



## Computing $\hat{\theta}_{GMM}$: 2-step GMM

One way to solve this problem is to apply a widely-used algorithm: the **2-step GMM**

. . .

**Step 1**

- Using $W = I$ (identity matrix), estimate $\hat{\theta}_1$
  - Alternatives exist, but we are going to stick with the simplest here
. . .

- With $\hat{\theta}_1$, calculate $\hat{W} = \left\{E[g(z_i; \hat{\theta}) g(z_i; \hat{\theta})^\prime]\right\}^{-1}$



## Computing $\hat{\theta}_{GMM}$: 2-step GMM

**Step 2**

- Using $\hat{W}$ from step 1, estimate $\hat{\theta}_{GMM}$

. . .

- Recalculate $\hat{S}^{-1} = \left\{E[g(z_i; \hat{\theta}) g(z_i; \hat{\theta})^\prime]\right\}^{-1}$
- Calculate $\hat{D} = \frac{1}{N} \sum_{i=1}^N \frac{\partial g(z_i; \hat{\theta}_{GMM})}{\partial \theta^\prime}$

. . .

- Then, calculate the asymptotic Variance-Covariance matrix $V(\hat{\theta}_{GMM}) = (\hat{D}^\prime \hat{S}^{-1} \hat{D})^{-1}$


# GMM hypothesis and specification tests

## Hypothesis tests for GMM

Again, we will consider a generic null hypothesis we want to test

$$
H_0: c(\theta) = 0
$$

There are three commonly used tests for GMM estimation that are analogous to the ones we covered for MLE.

- D-test
- Wald (W) test
- Lagrange Multiplier (LM) test


## Hypothesis tests: common assumptions

1. All the tests will assume we have GMM estimates using the efficient weighting matrix (estimated, for example, with the 2-step procedure).
2. The **unrestricted** GMM estimate $\hat\theta_u$ is estimated by minimizing the objective function $Q_N(\theta) = g_N(\theta)^\prime S_N^{-1} g_N(\theta)^\prime$
   - where $g_N(\theta) = N^{-1} \sum_i g(z_i;\theta)$ and $S_N$ is consistent for $S_0 = V[g_N(\theta)]$
3. The **restricted** GMM estimate $\hat\theta_r$ is estimated by minimizing the objective function $Q_N(\theta)$ subject to constraint $c(\theta) = 0$

## D-test

The D-test is analogous to the Likelihood Ratio as it is based on the differences of the objective functions when evaluated at the restricted and the unrestricted estimates. The D-statistics is given by

$$
D = N \left[Q_N(\hat\theta_r) - Q_N(\hat\theta_u)\right] \sim \chi^2_{[df]}
$$

where $df$ is again the number of restrictions imposed.

As in the LR test, we need to estimate both the unrestricted and the restricted models to obtain the D-statistic.

## Wald test

As in the MLE case, the Wald statistic is based on how close $c(\hat\theta)$ is to zero

$$
W = c(\hat\theta_u)^\prime V\left[c(\hat\theta_u\right]c(\hat\theta_u) \sim \chi^2_{[df]}
$$

where $df$ is again the number of restrictions imposed and $V(\hat{c})$ is the asymptotic variance of $c$, which now is given by

$$
V(c(\hat\theta)) = \left[\frac{\partial c(\hat\theta)}{\partial \hat\theta^\prime}\right]\left[V(\hat\theta)\right]^{-1}\left[\frac{\partial c(\hat\theta)}{\partial \hat\theta^\prime}\right]^\prime
$$

and $V(\hat{\theta}) = (\hat{D}^\prime \hat{S}^{-1} \hat{D})^{-1}$ and $\hat{D} = \frac{\partial g_N(\hat\theta)}{\partial \theta^\prime}$.


## LM test

The LM test is again based on how close the gradient of the objective function is to zero. The idea is that, if the restricted estimate is indeed valid, the gradients should approximately satisty the FOC.

This gives the LM statistics

$$
LM = N g(\hat\theta_r)^\prime  S^{-1} D(\hat\theta_r) \left[D(\hat\theta_r)^\prime S^{-1} D(\hat\theta_r)\right] D(\hat\theta_r)^\prime S^{-1} g(\hat\theta_r) \sim \chi^2_{[df]}
$$

where $S$, $D$ and $df$ are defined in the previous slides.

## Specification test: Overidentifying Restrictions

GMM has a general specification test that can be used when there are more moment conditions than parameters (i.e., an overidentified system).



# MLE vs GMM

## MLE vs GMM

It's the same old **bias (or robustness) vs. efficiency trade-off**

. . .

- With MLE, we need to make assumption on distributions of unobservables
  - When our assumptions are correct, MLE is more efficient $\rightarrow$ lower variance
  - Has good small sample properties (less bias, more efficiency with small data)
  - If our assumptions are inadequate, estimates are more biased
  
  
. . .

- With GMM, we don't need to assume distributions and can rely only on moment conditions from the theoretical and statistical model
  - This is more robust $=$ less bias
  - Has good large sample properties (less bias, more efficiency with large data)
  - But it's in general less efficient than MLE $\rightarrow$ higher variance



## Choosing between MLE and GMM

- *How much data is available?*
  - Large data sets favor GMM: good large sample properties require fewer assumptions. Smaller data sets might require stronger distributional assumptions $\rightarrow$ MLE

. . .

- *How complex is the model?*
  - MLE is better suited for linear and quadratic models, but technically difficult to compute with highly nonlinear models. For the latter case, GMM might be better

. . .

- *How comfortable are you making distributional assumptions?*
  - MLE requires you to fully specify distributions. If there is good theoretical grounding for these assumptions, MLE is a good idea. Otherwise, GMM is the more attractive option


